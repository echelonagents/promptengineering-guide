
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../02_large_language_models/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Natural Language Processing - Prompt Engineering Guide</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#natural-language-processing" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Prompt Engineering Guide" class="md-header__button md-logo" aria-label="Prompt Engineering Guide" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Prompt Engineering Guide
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Natural Language Processing
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Prompt Engineering Guide" class="md-nav__button md-logo" aria-label="Prompt Engineering Guide" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Prompt Engineering Guide
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#outline" class="md-nav__link">
    <span class="md-ellipsis">
      Outline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-fundamental-theory-and-concepts-of-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      1. Fundamental Theory and Concepts of NLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-developing-and-training-nlp-models" class="md-nav__link">
    <span class="md-ellipsis">
      2. Developing and Training NLP Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-overview-of-nlp-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      3. Overview of NLP Tasks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Overview of NLP Tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    <span class="md-ellipsis">
      Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-text-preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      4. Text Preprocessing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Text Preprocessing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      Code Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-classical-representations" class="md-nav__link">
    <span class="md-ellipsis">
      5. Classical Representations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Classical Representations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#n-grams" class="md-nav__link">
    <span class="md-ellipsis">
      N-grams
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-distributed-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      6. Distributed Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Distributed Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contextual-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Contextual Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      Training Word2vec
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-embedding-spaces-and-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      7. Embedding Spaces and Similarity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Embedding Spaces and Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vector-arithmetic" class="md-nav__link">
    <span class="md-ellipsis">
      Vector Arithmetic
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-semantic-search-and-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      8. Semantic Search and Clustering
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-evaluation-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      9. Evaluation Metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-machine-learning-with-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      10. Machine Learning with NLP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10. Machine Learning with NLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-unstructured-text" class="md-nav__link">
    <span class="md-ellipsis">
      Using Unstructured Text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokens-as-features" class="md-nav__link">
    <span class="md-ellipsis">
      Tokens as Features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification-and-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Classification and Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsupervised-and-semi-supervised-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised and Semi-supervised Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-applications" class="md-nav__link">
    <span class="md-ellipsis">
      Other Applications
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-practical-exercise" class="md-nav__link">
    <span class="md-ellipsis">
      11. Practical Exercise
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. Practical Exercise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sample-setup-code" class="md-nav__link">
    <span class="md-ellipsis">
      Sample Setup Code
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_large_language_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Large Language Models
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_rag_embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG, Embedding and Vector Databases
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_prompt_engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Engineering
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_ai_applications/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Applications
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_ai_agent_langchain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Agents using LangChain
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_llm_solutions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Solutions
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../08_case_example_txn_reconcile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Case Example Transaction Reconciliation
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#outline" class="md-nav__link">
    <span class="md-ellipsis">
      Outline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-fundamental-theory-and-concepts-of-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      1. Fundamental Theory and Concepts of NLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-developing-and-training-nlp-models" class="md-nav__link">
    <span class="md-ellipsis">
      2. Developing and Training NLP Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-overview-of-nlp-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      3. Overview of NLP Tasks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Overview of NLP Tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    <span class="md-ellipsis">
      Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-text-preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      4. Text Preprocessing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Text Preprocessing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      Code Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-classical-representations" class="md-nav__link">
    <span class="md-ellipsis">
      5. Classical Representations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Classical Representations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#n-grams" class="md-nav__link">
    <span class="md-ellipsis">
      N-grams
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-distributed-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      6. Distributed Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Distributed Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contextual-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Contextual Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      Training Word2vec
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-embedding-spaces-and-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      7. Embedding Spaces and Similarity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Embedding Spaces and Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vector-arithmetic" class="md-nav__link">
    <span class="md-ellipsis">
      Vector Arithmetic
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-semantic-search-and-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      8. Semantic Search and Clustering
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-evaluation-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      9. Evaluation Metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-machine-learning-with-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      10. Machine Learning with NLP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10. Machine Learning with NLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-unstructured-text" class="md-nav__link">
    <span class="md-ellipsis">
      Using Unstructured Text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokens-as-features" class="md-nav__link">
    <span class="md-ellipsis">
      Tokens as Features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification-and-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Classification and Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsupervised-and-semi-supervised-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised and Semi-supervised Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-applications" class="md-nav__link">
    <span class="md-ellipsis">
      Other Applications
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-practical-exercise" class="md-nav__link">
    <span class="md-ellipsis">
      11. Practical Exercise
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. Practical Exercise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sample-setup-code" class="md-nav__link">
    <span class="md-ellipsis">
      Sample Setup Code
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="natural-language-processing">Natural Language Processing</h1>
<p>Natural Language Processing (NLP) is the discipline focused on understanding and generating human language with computers. In our first week we build the foundational toolkit for working with raw text. The goal is to transform unstructured sentences into representations that can be fed into language models or traditional machine learning pipelines.</p>
<h2 id="outline">Outline</h2>
<p>The key topics for this module are:</p>
<ul>
<li>fundamental theory and language structure</li>
<li>developing and training NLP models</li>
<li>overview of typical NLP tasks</li>
<li>text preprocessing techniques</li>
<li>classical representations such as bag-of-words</li>
<li>distributed embeddings and contextual vectors</li>
<li>embedding spaces and similarity search</li>
<li>clustering and semantic retrieval</li>
<li>evaluation metrics for NLP pipelines</li>
<li>machine learning with NLP features</li>
<li>a hands-on practical exercise</li>
</ul>
<h2 id="1-fundamental-theory-and-concepts-of-nlp">1. Fundamental Theory and Concepts of NLP</h2>
<p>NLP builds upon linguistics, the scientific study of language. At the <strong>word</strong>
level we analyze morphology: how words are formed from roots and affixes. The
<strong>grammar</strong> or <strong>syntactic</strong> level concerns how words combine into valid
sentences. Finally the <strong>semantic</strong> level addresses meaning and pragmatic
interpretation. Understanding these structures is crucial for building models
that do more than pattern matching. Many NLP tasks—from part-of-speech tagging
to parsing—explicitly model these layers.</p>
<h2 id="2-developing-and-training-nlp-models">2. Developing and Training NLP Models</h2>
<p>Modern NLP pipelines rely on large text corpora. A corpus is collected and
cleaned before it is split into training, validation and test sets. Models are
trained to predict tokens, tags or embeddings using algorithms such as
conditional random fields or neural networks. During training we iterate over
the corpus, computing gradients and updating parameters until the model
generalizes well. Public corpora like Wikipedia or Common Crawl provide billions
of tokens, but domain-specific collections are equally important for applied
projects.</p>
<h2 id="3-overview-of-nlp-tasks">3. Overview of NLP Tasks</h2>
<p>NLP spans a broad range of tasks from tokenization and part-of-speech tagging to machine translation and text generation. We examine the classic pipeline of text classification, named entity recognition and summarization. Each task benefits from accurate preprocessing and token management, topics that we will explore in depth.</p>
<h3 id="example">Example</h3>
<p>A typical text classification workflow reads a document, splits it into tokens, removes non-essential words and converts the remaining terms into numerical features. These features are then fed into a classifier, such as logistic regression or a neural network, to predict sentiment or other labels.</p>
<h2 id="4-text-preprocessing">4. Text Preprocessing</h2>
<p>Before we can use text in downstream models, we clean and normalize it. Typical steps include lowercasing, removing punctuation and expanding contractions. Tokenization divides the text into units—either words or subwords—so that the model can assign numerical values.
- <strong>Normalization</strong>: Convert text to lowercase and standardize punctuation or white space.
- <strong>Tokenization</strong>: Using rules-based or statistical approaches, break text into words or subwords. Libraries such as spaCy or NLTK provide tokenizers for many languages.
- <strong>Stop Word Removal</strong>: Many analyses remove common words ("the", "and") to focus on informative terms.
- <strong>Stemming and Lemmatization</strong>: Reduce words to their root forms. Stemming uses heuristics to chop off endings, while lemmatization references dictionaries for correct morphological forms.</p>
<h3 id="code-example">Code Example</h3>
<pre><code class="language-python">import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(&quot;Cats running and dogs run&quot;)
print([token.lemma_ for token in doc])
</code></pre>
<h2 id="5-classical-representations">5. Classical Representations</h2>
<p>Early NLP models relied on bag-of-words features. Each document is represented by a vector that counts the occurrences of every term. TF‑IDF weighting scales the counts to emphasize distinctive words and downplay common terms. Despite their simplicity, these representations still perform well in some tasks.</p>
<h3 id="n-grams">N-grams</h3>
<p>To capture short phrases, we can use n-gram features. A bigram model represents consecutive word pairs; a trigram model uses three-word sequences. These features help with tasks like language detection and topic modeling.</p>
<h2 id="6-distributed-embeddings">6. Distributed Embeddings</h2>
<p>Neural embeddings offer dense representations that encode semantic similarity between words. Word2vec and GloVe learn vectors by predicting contexts or factorizing co-occurrence matrices. Similar words appear close together in vector space.</p>
<h3 id="contextual-embeddings">Contextual Embeddings</h3>
<p>Recent models such as ELMo and BERT produce token embeddings that depend on the sentence. A word like "bank" yields different vectors in "river bank" versus "bank account." These embeddings are the starting point for modern large language models.</p>
<h3 id="training-word2vec">Training Word2vec</h3>
<pre><code class="language-python">from gensim.models import Word2Vec
sentences = [['this','is','a','sentence'], ['this','is','another']]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)
vector = model.wv['sentence']
</code></pre>
<h2 id="7-embedding-spaces-and-similarity">7. Embedding Spaces and Similarity</h2>
<p>Once words or sentences are embedded into vectors, we can measure similarity with distances such as cosine or Euclidean metrics. Nearby vectors indicate semantic closeness. This property underpins search, clustering and recommendation systems.</p>
<h3 id="vector-arithmetic">Vector Arithmetic</h3>
<p>Word embeddings support analogies like <code>king - man + woman ≈ queen</code>. By subtracting and adding vectors we capture relationships between concepts.</p>
<h2 id="8-semantic-search-and-clustering">8. Semantic Search and Clustering</h2>
<p>To build a semantic search engine, we embed user queries and corpus documents into the same vector space. A similarity search (often using approximate nearest neighbors) retrieves documents with vectors nearest to the query vector. Clustering algorithms like K-means can reveal topical structure in the corpus.</p>
<h2 id="9-evaluation-metrics">9. Evaluation Metrics</h2>
<p>Common metrics for evaluating NLP pipelines include precision, recall and F1 score for classification, or BLEU and ROUGE for machine translation and summarization. Proper evaluation helps compare algorithms and tune hyperparameters.</p>
<h2 id="10-machine-learning-with-nlp">10. Machine Learning with NLP</h2>
<p>Machine learning techniques turn processed text into actionable predictions.
Below we examine how unstructured data is transformed into features and how
those features drive various algorithms.</p>
<h3 id="using-unstructured-text">Using Unstructured Text</h3>
<p>Raw documents must be tokenized and vectorized before they can feed an ML
pipeline. Common approaches include bag-of-words counts, TF‑IDF weights and
pretrained embeddings. These representations transform arbitrary length text
into fixed-length numeric vectors suitable for scikit-learn or deep learning
frameworks.</p>
<h3 id="tokens-as-features">Tokens as Features</h3>
<p>Tokens or n‑grams can be treated as categorical variables. With large vocabularies
we typically apply hashing or dimensionality reduction to keep feature spaces
manageable. Embeddings offer dense alternatives that capture semantic meaning
beyond surface forms.</p>
<h3 id="classification-and-regression">Classification and Regression</h3>
<p>Text classification predicts discrete labels such as sentiment or topic. In a
regression setup the target might be a review score or popularity metric.
Logistic regression, support vector machines and gradient boosting all work well
with TF‑IDF or embedding features. Neural architectures like CNNs or transformers
handle longer sequences directly and can be fine-tuned on domain corpora.</p>
<h3 id="unsupervised-and-semi-supervised-methods">Unsupervised and Semi-supervised Methods</h3>
<p>Clustering groups documents by similarity without requiring labels. Topic models
such as LDA reveal latent structure, while autoencoders learn compressed
representations. Semi-supervised techniques leverage small labeled datasets with
large amounts of unlabeled text to improve performance.</p>
<h3 id="other-applications">Other Applications</h3>
<p>Sequence tagging tasks like part-of-speech or entity recognition combine
contextual embeddings with conditional random fields or recurrent networks.
Generative models perform machine translation or summarization. All of these
approaches rely on the same principle—encoding language into numerical features
and optimizing a learning objective.</p>
<h2 id="11-practical-exercise">11. Practical Exercise</h2>
<p>The assignment for this module is to implement a small semantic search engine. Using Python libraries such as spaCy and scikit-learn, you will:
1. Preprocess a text dataset by tokenizing, lemmatizing and removing stop words.
2. Generate TF‑IDF features and train a basic classifier.
3. Create sentence embeddings using a transformer model like <code>all-MiniLM</code> from the <code>sentence-transformers</code> package.
4. Perform a similarity query and evaluate the retrieval quality.</p>
<h3 id="sample-setup-code">Sample Setup Code</h3>
<pre><code class="language-python">from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-MiniLM-L6-v2')
corpus = [&quot;NLP is fun&quot;, &quot;We study language&quot;, &quot;Transformers are powerful&quot;]
corpus_embeddings = model.encode(corpus, convert_to_tensor=True)
query = &quot;language models&quot;
query_embedding = model.encode(query, convert_to_tensor=True)
results = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)
print(results)
</code></pre>
<p>By the end of week one, you should be comfortable preparing text for advanced models, generating embeddings and measuring similarity. These skills are essential for all subsequent weeks of the course.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>