{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prompt Engineering Guide","text":"<p>This guide provides information for Prompt Engineering and working with LLMs. Each topic delves into theory, data structures and practical algorithms so you can build end\u2011to\u2011end solutions.</p> <p>Each topic page includes a Contents Outline section at the beginning so you can quickly navigate the material and locate key subtopics.</p>"},{"location":"#topics","title":"Topics","text":"<p>The course focuses on:</p> <ul> <li>Natural Language Processing (NLP)</li> <li>AI application development</li> <li>Solutions driven by large language models (LLM)</li> <li>Large Language Models: GPT-3/4, Claude, etc.</li> <li>Prompt Engineering concepts</li> </ul>"},{"location":"#course-outline","title":"Course Outline","text":"<ul> <li>Natural Language Processing</li> <li>fundamentals of text processing and tokenization</li> <li>embeddings and semantic understanding</li> <li>Large Language Models</li> <li>overview of GPT-3/4, Claude and other providers</li> <li>transformer architecture basics</li> <li>using APIs for inference and evaluation</li> <li>RAG, Embedding and Vector Databases</li> <li>retrieval augmented generation workflows</li> <li>embedding models for semantic search</li> <li>storing and querying vectors in databases</li> <li>Prompt Engineering</li> <li>prompt design best practices</li> <li>zero-shot, few-shot and chain-of-thought techniques</li> <li>system versus user prompts</li> <li>AI Applications</li> <li>integrating LLM APIs into applications</li> <li>AI Agents using LangChain</li> <li>chains, tools and memory management</li> <li>building conversational agents</li> <li>orchestrating multi-step reasoning</li> <li>LLM Solutions</li> <li>building chatbots and conversational agents</li> <li>summarization and question answering</li> <li>domain-specific automation examples</li> <li>Case Example: Transaciton Reconciliation A case examples of RPA and LLM AI Agents applied to real world use case of transaction reconciliation</li> </ul>"},{"location":"#course-schedule","title":"Course Schedule","text":"<p>Week 1: Natural Language Processing Week 2: Large Language Models Week 3: Retrieval Augmented Generation and Embedding Week 4: Prompt Engineering Week 5: AI Applications Week 6: AI Agent Development using LangChain Week 7: LLM Solutions and Project Showcase</p>"},{"location":"01_natural_language_processing/","title":"Natural Language Processing","text":"<p>Natural Language Processing (NLP) is the discipline focused on understanding and generating human language with computers. In our first week we build the foundational toolkit for working with raw text. The goal is to transform unstructured sentences into representations that can be fed into language models or traditional machine learning pipelines.</p>"},{"location":"01_natural_language_processing/#outline","title":"Outline","text":"<p>The key topics for this module are:</p> <ul> <li>fundamental theory and language structure</li> <li>developing and training NLP models</li> <li>overview of typical NLP tasks</li> <li>text preprocessing techniques</li> <li>classical representations such as bag-of-words</li> <li>distributed embeddings and contextual vectors</li> <li>embedding spaces and similarity search</li> <li>clustering and semantic retrieval</li> <li>evaluation metrics for NLP pipelines</li> <li>machine learning with NLP features</li> <li>a hands-on practical exercise</li> </ul>"},{"location":"01_natural_language_processing/#1-fundamental-theory-and-concepts-of-nlp","title":"1. Fundamental Theory and Concepts of NLP","text":"<p>NLP builds upon linguistics, the scientific study of language. At the word level we analyze morphology: how words are formed from roots and affixes. The grammar or syntactic level concerns how words combine into valid sentences. Finally the semantic level addresses meaning and pragmatic interpretation. Understanding these structures is crucial for building models that do more than pattern matching. Many NLP tasks\u2014from part-of-speech tagging to parsing\u2014explicitly model these layers.</p>"},{"location":"01_natural_language_processing/#2-developing-and-training-nlp-models","title":"2. Developing and Training NLP Models","text":"<p>Modern NLP pipelines rely on large text corpora. A corpus is collected and cleaned before it is split into training, validation and test sets. Models are trained to predict tokens, tags or embeddings using algorithms such as conditional random fields or neural networks. During training we iterate over the corpus, computing gradients and updating parameters until the model generalizes well. Public corpora like Wikipedia or Common Crawl provide billions of tokens, but domain-specific collections are equally important for applied projects.</p>"},{"location":"01_natural_language_processing/#3-overview-of-nlp-tasks","title":"3. Overview of NLP Tasks","text":"<p>NLP spans a broad range of tasks from tokenization and part-of-speech tagging to machine translation and text generation. We examine the classic pipeline of text classification, named entity recognition and summarization. Each task benefits from accurate preprocessing and token management, topics that we will explore in depth.</p>"},{"location":"01_natural_language_processing/#example","title":"Example","text":"<p>A typical text classification workflow reads a document, splits it into tokens, removes non-essential words and converts the remaining terms into numerical features. These features are then fed into a classifier, such as logistic regression or a neural network, to predict sentiment or other labels.</p>"},{"location":"01_natural_language_processing/#4-text-preprocessing","title":"4. Text Preprocessing","text":"<p>Before we can use text in downstream models, we clean and normalize it. Typical steps include lowercasing, removing punctuation and expanding contractions. Tokenization divides the text into units\u2014either words or subwords\u2014so that the model can assign numerical values. - Normalization: Convert text to lowercase and standardize punctuation or white space. - Tokenization: Using rules-based or statistical approaches, break text into words or subwords. Libraries such as spaCy or NLTK provide tokenizers for many languages. - Stop Word Removal: Many analyses remove common words (\"the\", \"and\") to focus on informative terms. - Stemming and Lemmatization: Reduce words to their root forms. Stemming uses heuristics to chop off endings, while lemmatization references dictionaries for correct morphological forms.</p>"},{"location":"01_natural_language_processing/#code-example","title":"Code Example","text":"<pre><code>import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"Cats running and dogs run\")\nprint([token.lemma_ for token in doc])\n</code></pre>"},{"location":"01_natural_language_processing/#5-classical-representations","title":"5. Classical Representations","text":"<p>Early NLP models relied on bag-of-words features. Each document is represented by a vector that counts the occurrences of every term. TF\u2011IDF weighting scales the counts to emphasize distinctive words and downplay common terms. Despite their simplicity, these representations still perform well in some tasks.</p>"},{"location":"01_natural_language_processing/#n-grams","title":"N-grams","text":"<p>To capture short phrases, we can use n-gram features. A bigram model represents consecutive word pairs; a trigram model uses three-word sequences. These features help with tasks like language detection and topic modeling.</p>"},{"location":"01_natural_language_processing/#6-distributed-embeddings","title":"6. Distributed Embeddings","text":"<p>Neural embeddings offer dense representations that encode semantic similarity between words. Word2vec and GloVe learn vectors by predicting contexts or factorizing co-occurrence matrices. Similar words appear close together in vector space.</p>"},{"location":"01_natural_language_processing/#contextual-embeddings","title":"Contextual Embeddings","text":"<p>Recent models such as ELMo and BERT produce token embeddings that depend on the sentence. A word like \"bank\" yields different vectors in \"river bank\" versus \"bank account.\" These embeddings are the starting point for modern large language models.</p>"},{"location":"01_natural_language_processing/#training-word2vec","title":"Training Word2vec","text":"<pre><code>from gensim.models import Word2Vec\nsentences = [['this','is','a','sentence'], ['this','is','another']]\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\nvector = model.wv['sentence']\n</code></pre>"},{"location":"01_natural_language_processing/#7-embedding-spaces-and-similarity","title":"7. Embedding Spaces and Similarity","text":"<p>Once words or sentences are embedded into vectors, we can measure similarity with distances such as cosine or Euclidean metrics. Nearby vectors indicate semantic closeness. This property underpins search, clustering and recommendation systems.</p>"},{"location":"01_natural_language_processing/#vector-arithmetic","title":"Vector Arithmetic","text":"<p>Word embeddings support analogies like <code>king - man + woman \u2248 queen</code>. By subtracting and adding vectors we capture relationships between concepts.</p>"},{"location":"01_natural_language_processing/#8-semantic-search-and-clustering","title":"8. Semantic Search and Clustering","text":"<p>To build a semantic search engine, we embed user queries and corpus documents into the same vector space. A similarity search (often using approximate nearest neighbors) retrieves documents with vectors nearest to the query vector. Clustering algorithms like K-means can reveal topical structure in the corpus.</p>"},{"location":"01_natural_language_processing/#9-evaluation-metrics","title":"9. Evaluation Metrics","text":"<p>Common metrics for evaluating NLP pipelines include precision, recall and F1 score for classification, or BLEU and ROUGE for machine translation and summarization. Proper evaluation helps compare algorithms and tune hyperparameters.</p>"},{"location":"01_natural_language_processing/#10-machine-learning-with-nlp","title":"10. Machine Learning with NLP","text":"<p>Machine learning techniques turn processed text into actionable predictions. Below we examine how unstructured data is transformed into features and how those features drive various algorithms.</p>"},{"location":"01_natural_language_processing/#using-unstructured-text","title":"Using Unstructured Text","text":"<p>Raw documents must be tokenized and vectorized before they can feed an ML pipeline. Common approaches include bag-of-words counts, TF\u2011IDF weights and pretrained embeddings. These representations transform arbitrary length text into fixed-length numeric vectors suitable for scikit-learn or deep learning frameworks.</p>"},{"location":"01_natural_language_processing/#tokens-as-features","title":"Tokens as Features","text":"<p>Tokens or n\u2011grams can be treated as categorical variables. With large vocabularies we typically apply hashing or dimensionality reduction to keep feature spaces manageable. Embeddings offer dense alternatives that capture semantic meaning beyond surface forms.</p>"},{"location":"01_natural_language_processing/#classification-and-regression","title":"Classification and Regression","text":"<p>Text classification predicts discrete labels such as sentiment or topic. In a regression setup the target might be a review score or popularity metric. Logistic regression, support vector machines and gradient boosting all work well with TF\u2011IDF or embedding features. Neural architectures like CNNs or transformers handle longer sequences directly and can be fine-tuned on domain corpora.</p>"},{"location":"01_natural_language_processing/#unsupervised-and-semi-supervised-methods","title":"Unsupervised and Semi-supervised Methods","text":"<p>Clustering groups documents by similarity without requiring labels. Topic models such as LDA reveal latent structure, while autoencoders learn compressed representations. Semi-supervised techniques leverage small labeled datasets with large amounts of unlabeled text to improve performance.</p>"},{"location":"01_natural_language_processing/#other-applications","title":"Other Applications","text":"<p>Sequence tagging tasks like part-of-speech or entity recognition combine contextual embeddings with conditional random fields or recurrent networks. Generative models perform machine translation or summarization. All of these approaches rely on the same principle\u2014encoding language into numerical features and optimizing a learning objective.</p>"},{"location":"01_natural_language_processing/#11-practical-exercise","title":"11. Practical Exercise","text":"<p>The assignment for this module is to implement a small semantic search engine. Using Python libraries such as spaCy and scikit-learn, you will: 1. Preprocess a text dataset by tokenizing, lemmatizing and removing stop words. 2. Generate TF\u2011IDF features and train a basic classifier. 3. Create sentence embeddings using a transformer model like <code>all-MiniLM</code> from the <code>sentence-transformers</code> package. 4. Perform a similarity query and evaluate the retrieval quality.</p>"},{"location":"01_natural_language_processing/#sample-setup-code","title":"Sample Setup Code","text":"<pre><code>from sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ncorpus = [\"NLP is fun\", \"We study language\", \"Transformers are powerful\"]\ncorpus_embeddings = model.encode(corpus, convert_to_tensor=True)\nquery = \"language models\"\nquery_embedding = model.encode(query, convert_to_tensor=True)\nresults = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)\nprint(results)\n</code></pre> <p>By the end of week one, you should be comfortable preparing text for advanced models, generating embeddings and measuring similarity. These skills are essential for all subsequent weeks of the course.</p>"},{"location":"02_large_language_models/","title":"Large Language Models","text":"<p>Modern large language models (LLMs) such as GPT-4, Claude and open-source alternatives are built upon the transformer architecture. This week provides a deep dive into how these models are trained and how we can use them effectively.</p>"},{"location":"02_large_language_models/#outline","title":"Outline","text":"<ul> <li>model families and providers</li> <li>transformer architecture basics</li> <li>tokenization and vocabulary</li> <li>training algorithms</li> <li>using hosted APIs</li> <li>evaluation and fine-tuning</li> <li>ethical and practical concerns</li> <li>hands-on code example</li> </ul>"},{"location":"02_large_language_models/#1-model-families","title":"1. Model Families","text":"<p>LLM providers fall into two broad categories: proprietary offerings like OpenAI\u2019s GPT series or Anthropic\u2019s Claude, and open models such as Llama or Mistral. Each family offers different licensing terms and API interfaces, but the underlying neural architecture shares common principles.</p>"},{"location":"02_large_language_models/#proprietary-models","title":"Proprietary Models","text":"<ul> <li>GPT-3/4: Developed by OpenAI, these models are known for their extensive training data and high-quality generation.</li> <li>Claude: Anthropic\u2019s assistant focuses on safety and steerable responses.</li> </ul>"},{"location":"02_large_language_models/#open-source-alternatives","title":"Open Source Alternatives","text":"<ul> <li>Llama from Meta and Mistral from independent labs show that smaller, efficient models can achieve competitive performance when fine-tuned on domain-specific data. Running open models locally gives greater control over data privacy and cost.</li> </ul>"},{"location":"02_large_language_models/#2-transformer-architecture","title":"2. Transformer Architecture","text":"<p>The transformer revolutionized NLP by introducing self-attention mechanisms. Each layer computes queries, keys and values to weigh the importance of surrounding tokens. Positional encodings supply order information, while residual connections and layer normalization aid optimization.</p>"},{"location":"02_large_language_models/#autoregressive-training","title":"Autoregressive Training","text":"<p>LLMs are typically trained to predict the next token in a sequence. This objective encourages the model to learn grammar, semantics and even factual knowledge from the training corpus.</p>"},{"location":"02_large_language_models/#scaling-laws","title":"Scaling Laws","text":"<p>Research shows that model performance scales predictably with data, parameter count and compute budget. Understanding these relationships helps when deciding between a smaller local model and a large hosted service.</p>"},{"location":"02_large_language_models/#3-tokenization-and-vocabulary","title":"3. Tokenization and Vocabulary","text":"<p>Tokenizers split text into manageable pieces. Byte-Pair Encoding (BPE) and WordPiece are common algorithms that build a vocabulary of subword units. Each token is mapped to an integer ID so it can be processed by the model. The vocabulary typically includes special symbols like <code>&lt;pad&gt;</code> for padding sequences and <code>&lt;eos&gt;</code> for marking the end of a sentence. Proper tokenization yields consistent sequence lengths and reduces out-of-vocabulary issues.</p>"},{"location":"02_large_language_models/#4-training-algorithms","title":"4. Training Algorithms","text":"<p>Training begins with massive text corpora that are shuffled and broken into fixed-length sequences. The transformer processes each batch while the optimizer updates weights via gradient descent. Strategies such as gradient accumulation, mixed-precision training and data parallelism enable scaling to billions of parameters. Many teams further align the model with Reinforcement Learning from Human Feedback (RLHF), which uses human preference scores and Proximal Policy Optimization to refine responses.</p>"},{"location":"02_large_language_models/#5-using-hosted-apis","title":"5. Using Hosted APIs","text":"<p>Most developers access LLMs through web APIs. When sending prompts, it\u2019s important to manage context length, system instructions and rate limits. Providers like OpenAI return responses in JSON format with metadata. Error handling ensures robust applications.</p>"},{"location":"02_large_language_models/#example","title":"Example","text":"<pre><code>import openai\nopenai.api_key = \"YOUR_KEY\"\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain transformers\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"02_large_language_models/#tips","title":"Tips","text":"<ul> <li>Cache frequent prompts to reduce latency and cost.</li> <li>Monitor token usage to avoid unexpected bills.</li> <li>Evaluate model outputs for bias or inappropriate content.</li> </ul>"},{"location":"02_large_language_models/#6-evaluation-and-fine-tuning","title":"6. Evaluation and Fine-tuning","text":"<p>While base models provide impressive capabilities, performance often improves with fine-tuning on task-specific data. Evaluation metrics such as perplexity, BLEU or human preference scores help determine whether fine-tuning is necessary.</p>"},{"location":"02_large_language_models/#parameter-efficient-methods","title":"Parameter-Efficient Methods","text":"<p>Techniques like LoRA (Low-Rank Adaptation) and prompt tuning modify only a small subset of model parameters or the input embeddings, enabling efficient adaptation even on modest hardware.</p>"},{"location":"02_large_language_models/#7-ethical-and-practical-concerns","title":"7. Ethical and Practical Concerns","text":"<p>Large models inherit biases present in their training data. It is critical to monitor for unfair or harmful outputs. Data privacy must be considered when sending user content to hosted services. Finally, the energy consumption required to train these models raises environmental concerns. Organizations should weigh these factors when deciding on adoption.</p>"},{"location":"02_large_language_models/#8-hands-on-code-example","title":"8. Hands-on Code Example","text":"<p>The snippet below shows how a small transformer model can be fine-tuned using the <code>transformers</code> library from Hugging Face. <pre><code>from datasets import load_dataset\nfrom transformers import (AutoTokenizer, AutoModelForCausalLM,\n                          Trainer, TrainingArguments)\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\nmodel = AutoModelForCausalLM.from_pretrained('gpt2')\n\ndef tokenize(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length')\n\ndata = load_dataset('wikitext', 'wikitext-2-raw-v1')\ntokenized = data['train'].map(tokenize, batched=True)\n\nargs = TrainingArguments('out', per_device_train_batch_size=2, num_train_epochs=1)\ntrainer = Trainer(model=model, args=args, train_dataset=tokenized)\ntrainer.train()\n</code></pre></p> <p>By the end of week two you will be able to describe the architecture of modern LLMs, interact with them via APIs and understand the trade-offs between proprietary and open-source solutions.</p>"},{"location":"03_rag_embedding/","title":"RAG, Embedding and Vector Databases","text":"<p>Retrieval Augmented Generation (RAG) combines the reasoning abilities of language models with the accuracy of external knowledge bases. In week three we learn to build systems that fetch relevant context and feed it to a generator model.</p>"},{"location":"03_rag_embedding/#outline","title":"Outline","text":"<ul> <li>the RAG workflow</li> <li>embedding models</li> <li>vector databases</li> <li>similarity search algorithms</li> <li>building a RAG system</li> <li>evaluation metrics</li> <li>challenges and best practices</li> <li>implementation exercise</li> </ul>"},{"location":"03_rag_embedding/#1-the-rag-workflow","title":"1. The RAG Workflow","text":"<ol> <li>Query Encoding: The user question or prompt is embedded using a transformer encoder.</li> <li>Vector Retrieval: The embedding is compared to vectors stored in a database. The most similar passages are retrieved.</li> <li>Generation: The retrieved text chunks are provided to a language model which produces a final answer grounded in that context.</li> </ol> <p>By retrieving supporting evidence, RAG reduces hallucination and enables up-to-date information without retraining the model.</p>"},{"location":"03_rag_embedding/#2-embedding-models","title":"2. Embedding Models","text":"<p>Sentence transformers (e.g., <code>all-MiniLM</code> or <code>bge-large</code>) convert text into high-dimensional vectors. These vectors capture semantic meaning, allowing similarity search. Fine-tuning on domain data improves retrieval quality.</p> <p>Other embedding approaches include dual-encoder models where questions and documents are encoded separately, and cross-encoders that jointly score a query-document pair. Dual-encoders scale well to large corpora while cross-encoders can provide higher accuracy when reranking the top retrieved results.</p>"},{"location":"03_rag_embedding/#training-strategies","title":"Training Strategies","text":"<ul> <li>Unsupervised: Use contrastive learning on large corpora to build general-purpose embeddings.</li> <li>Supervised: Train on question\u2013answer pairs or labelled data for more accurate retrieval.</li> </ul>"},{"location":"03_rag_embedding/#3-vector-databases","title":"3. Vector Databases","text":"<p>Specialized databases store millions of embeddings and provide efficient similarity search. Popular options include: - FAISS: A library for indexing and searching high-dimensional vectors. It offers product quantization and GPU acceleration. - Qdrant and Weaviate: Standalone services that store vectors with associated metadata. They support filtering, sharding and persistence.</p>"},{"location":"03_rag_embedding/#indexing","title":"Indexing","text":"<p>Vectors are often compressed or quantized for speed. Inverted file (IVF) indexes and HNSW graphs allow approximate nearest-neighbor search with controllable accuracy.</p>"},{"location":"03_rag_embedding/#4-similarity-search-algorithms","title":"4. Similarity Search Algorithms","text":"<p>Vector search relies on distance metrics such as cosine or Euclidean similarity. Large datasets use approximate nearest-neighbor (ANN) methods to keep latency low. Algorithms like HNSW build a navigable small-world graph, while IVF splits vectors into clusters for coarse-to-fine retrieval. Choosing the right index depends on trade-offs between speed, memory and recall.</p>"},{"location":"03_rag_embedding/#data-structures","title":"Data Structures","text":"<ul> <li>HNSW Graphs organize vectors as layered graphs with short- and long-range   links to enable logarithmic search complexity.</li> <li>Inverted File Lists partition vectors into buckets for efficient scanning   of a subset of the dataset.</li> </ul>"},{"location":"03_rag_embedding/#5-building-a-rag-system","title":"5. Building a RAG System","text":"<ol> <li>Chunk documents into passages (e.g., 200\u2013300 words).</li> <li>Embed each chunk and store it in a vector database along with metadata.</li> <li>When a query arrives, encode it and retrieve the top-k similar chunks.</li> <li>Assemble a prompt that includes the retrieved context and ask the language model to answer based only on that information.</li> </ol>"},{"location":"03_rag_embedding/#example-code-snippet","title":"Example Code Snippet","text":"<pre><code>from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\nembed = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\ntexts = [\"Document chunk one\", \"Another chunk\"]\nstore = FAISS.from_texts(texts, embed)\nquestion = \"What is in the document?\"\nmatched_docs = store.similarity_search(question)\n</code></pre>"},{"location":"03_rag_embedding/#6-evaluation-metrics","title":"6. Evaluation Metrics","text":"<p>To gauge retrieval quality we measure recall\u2014the proportion of relevant documents returned\u2014as well as precision and overall latency. Evaluating the end-to-end RAG pipeline may involve human judgement of answer correctness or automatic metrics such as ROUGE for summarization tasks.</p>"},{"location":"03_rag_embedding/#7-challenges-and-best-practices","title":"7. Challenges and Best Practices","text":"<ul> <li>Chunking Strategy: Overlapping windows can preserve context but increase storage.</li> <li>Metadata: Storing source links allows the generation step to cite references.</li> <li>Freshness: Periodically re-embed new documents so the knowledge base stays current.</li> </ul>"},{"location":"03_rag_embedding/#8-implementation-exercise","title":"8. Implementation Exercise","text":"<p>Create a simple question-answering system that reads a set of articles, embeds them into a FAISS index and uses an LLM to respond to user questions with retrieved passages. Evaluate how retrieval quality changes when using different embedding models or index parameters.</p> <p>By completing week three you will know how to integrate vector search with language models to build applications that provide grounded, accurate answers.</p> <p>Further reading includes papers on Dense Passage Retrieval and RAG-based open-domain question answering. Experiment with different embedding models and index parameters to understand the trade-offs between accuracy and latency.</p>"},{"location":"04_prompt_engineering/","title":"Prompt Engineering","text":"<p>Week four explores how to craft prompts that reliably steer language models toward the desired output. Well-designed prompts can dramatically improve accuracy and reduce unwanted behavior.</p>"},{"location":"04_prompt_engineering/#outline","title":"Outline","text":"<ul> <li>prompt design principles</li> <li>zero-shot, few-shot and chain-of-thought</li> <li>using system prompts</li> <li>prompt templates and data structures</li> <li>advanced formatting</li> <li>prompt iteration and evaluation</li> <li>automated optimization</li> <li>practical exercise</li> </ul>"},{"location":"04_prompt_engineering/#1-prompt-design-principles","title":"1. Prompt Design Principles","text":"<p>A prompt should clearly state the task and provide any needed context. Typically we include a system or persona message to guide style and tone. Explicit instructions about output format help prevent confusion.</p>"},{"location":"04_prompt_engineering/#guidelines","title":"Guidelines","text":"<ul> <li>Keep instructions concise yet specific.</li> <li>Use delimiters such as triple backticks to separate instructions from user content.</li> <li>Include examples of correct output when possible.</li> <li>Control randomness with the temperature and top\u2011p parameters.</li> </ul>"},{"location":"04_prompt_engineering/#2-zero-shot-few-shot-and-chain-of-thought","title":"2. Zero-shot, Few-shot and Chain-of-Thought","text":"<ul> <li>Zero-shot: Ask the model to perform the task directly with no examples.</li> <li>Few-shot: Provide a handful of labeled examples within the prompt to establish the pattern.</li> <li>Chain-of-thought: Encourage multi-step reasoning by telling the model to explain its steps before giving the final answer.</li> </ul>"},{"location":"04_prompt_engineering/#self-consistency","title":"Self-consistency","text":"<p>By generating several chain-of-thought responses and voting on the most consistent outcome, we can achieve higher accuracy on complex reasoning tasks.</p>"},{"location":"04_prompt_engineering/#3-using-system-prompts","title":"3. Using System Prompts","text":"<p>System prompts establish the role or persona of the assistant, such as \"You are a helpful travel planner.\" Distinguishing between system and user prompts prevents instructions from being interpreted as user input.</p>"},{"location":"04_prompt_engineering/#controlling-style","title":"Controlling Style","text":"<p>We can mimic styles ranging from academic explanations to casual conversation. Including explicit style cues helps tailor the voice to the target audience.</p>"},{"location":"04_prompt_engineering/#4-prompt-templates-and-data-structures","title":"4. Prompt Templates and Data Structures","text":"<p>Complex applications store prompts as templates with placeholders. A template may be expressed as a Python string with <code>{variable}</code> tokens or as a structured object containing fields for system, user and tool messages. Managing prompts as data enables version control and automated testing.</p>"},{"location":"04_prompt_engineering/#example-template","title":"Example Template","text":"<pre><code>template = \"\"\"You are a math tutor.\\nQuestion: {question}\\nAnswer:\"\"\"\nfilled = template.format(question=\"What is 2+2?\")\n</code></pre>"},{"location":"04_prompt_engineering/#5-advanced-formatting","title":"5. Advanced Formatting","text":"<p>When output needs to be machine-readable, we can instruct the model to respond in JSON or Markdown. Using placeholders and bullet lists reduces variation in the output structure.</p>"},{"location":"04_prompt_engineering/#example-prompt","title":"Example Prompt","text":"<pre><code>You are a concise consultant.\nAnswer the question using three bullet points.\nQuestion: How can I reduce latency in a web app?\n</code></pre>"},{"location":"04_prompt_engineering/#temperature-and-top-p","title":"Temperature and Top-p","text":"<p>The randomness of model output is controlled with the <code>temperature</code> and <code>top_p</code> parameters. Lower temperature produces more deterministic responses, while a higher value encourages creative or varied text. Nucleus sampling (<code>top_p</code>) limits generation to the most probable tokens whose cumulative probability is below a threshold, often producing coherent yet diverse answers.</p>"},{"location":"04_prompt_engineering/#6-prompt-iteration-and-evaluation","title":"6. Prompt Iteration and Evaluation","text":"<p>Developing a good prompt is an iterative process. Start with a clear baseline, test the output on multiple examples and refine. Automated evaluation frameworks such as OpenAI\u2019s prompt engineering tools or the <code>langchain</code> evaluator can assist with this process.</p> <p>Evaluation often measures metrics like exact match on benchmark tasks or user satisfaction collected through ratings. Tracking how prompts perform across multiple criteria helps identify trade-offs between brevity, accuracy and style. Version control of prompts enables A/B testing and regression checks.</p>"},{"location":"04_prompt_engineering/#7-automated-optimization","title":"7. Automated Optimization","text":"<p>Tools such as prompt matrices and evolutionary algorithms can automatically test variations of a base prompt. By measuring accuracy or user satisfaction we can select the best-performing template. This approach scales when manual iteration becomes impractical.</p> <p>One common strategy is to randomly sample variations of wording or ordering within a prompt and score them with a reference answer. Gradient-free optimization methods such as Bayesian search can then converge on high-performing prompts with fewer trials.</p>"},{"location":"04_prompt_engineering/#8-practical-exercise","title":"8. Practical Exercise","text":"<p>Design prompts for a summarization tool and a data extraction pipeline. Experiment with different temperature settings and evaluate how formatting instructions affect the consistency of the model\u2019s responses.</p> <p>By the end of week four you will be able to write prompts that consistently guide LLMs to produce high-quality, safe and formatted output for a variety of tasks.</p> <p>Continuous improvement is key. Save successful prompts in a library and track their performance on real user queries. Over time these prompt libraries become valuable assets that accelerate development of new LLM applications.</p>"},{"location":"05_ai_applications/","title":"AI Applications","text":"<p>Week five focuses on integrating language models into real-world applications. We explore common patterns and discuss how to evaluate systems in production.</p>"},{"location":"05_ai_applications/#outline","title":"Outline","text":"<ul> <li>application patterns</li> <li>system architecture and data pipelines</li> <li>evaluation and feedback</li> <li>safety considerations</li> <li>monitoring and logging</li> <li>implementation exercise</li> </ul>"},{"location":"05_ai_applications/#1-application-patterns","title":"1. Application Patterns","text":"<p>LLMs power a variety of services including chatbots, summarization tools and knowledge extraction pipelines. We survey each of these patterns and discuss the design considerations involved.</p>"},{"location":"05_ai_applications/#chatbots-and-assistants","title":"Chatbots and Assistants","text":"<p>Conversational agents require state management to maintain context across turns. Techniques such as conversation memory or storing user history in a database help create natural, continuous interactions.</p>"},{"location":"05_ai_applications/#summarization-services","title":"Summarization Services","text":"<p>Automatic summarization condenses large documents into key points. Whether abstractive or extractive, summarizers rely on well-crafted prompts and often incorporate feedback from human reviewers.</p>"},{"location":"05_ai_applications/#knowledge-extraction","title":"Knowledge Extraction","text":"<p>Businesses use LLMs to identify entities and relationships within text. By combining extraction prompts with downstream databases, we can build structured knowledge bases from unstructured sources.</p>"},{"location":"05_ai_applications/#2-system-architecture-and-data-pipelines","title":"2. System Architecture and Data Pipelines","text":"<p>LLM applications often follow a microservice design. A front-end interface sends requests to a back-end service that orchestrates calls to the language model and any retrieval databases. Message queues or streaming systems pass conversation history and user feedback to analytics components. Choosing whether to call a hosted API or deploy a model locally depends on latency, cost and privacy.</p>"},{"location":"05_ai_applications/#data-schemas","title":"Data Schemas","text":"<p>Storing conversations typically involves a table of messages with columns for timestamp, user ID and text. Vector databases may store embeddings alongside the raw documents for retrieval.</p>"},{"location":"05_ai_applications/#3-evaluation-and-feedback","title":"3. Evaluation and Feedback","text":"<p>Deploying an LLM-powered app is an iterative process. We should monitor metrics such as response quality, latency and user satisfaction. A/B testing different prompt templates or model parameters helps determine what works best. Collecting explicit user feedback allows continuous improvement.</p> <p>Quantitative evaluation includes measuring perplexity or other automated metrics, while qualitative studies gather insights from domain experts. Maintaining test datasets with expected answers ensures that new model versions do not regress on established functionality.</p>"},{"location":"05_ai_applications/#human-in-the-loop","title":"Human-in-the-Loop","text":"<p>In sensitive domains, humans review model outputs before final delivery. This ensures accuracy and prevents harmful content from reaching end users.</p>"},{"location":"05_ai_applications/#4-safety-considerations","title":"4. Safety Considerations","text":"<p>LLMs can hallucinate or produce biased text. Implement guardrails such as content filters, rate limiting and logging. Privacy is also vital\u2014avoid sending sensitive user data to external services unless it has been properly anonymized and consent has been obtained.</p>"},{"location":"05_ai_applications/#5-monitoring-and-logging","title":"5. Monitoring and Logging","text":"<p>Production systems should log every request and response for troubleshooting and analysis. Metrics such as response time, token usage and user satisfaction scores help diagnose regressions. Dashboards enable operators to spot spikes in latency or abnormal output.</p>"},{"location":"05_ai_applications/#6-implementation-exercise","title":"6. Implementation Exercise","text":"<p>Design a simple application that integrates a chat interface with an LLM API. Track metrics such as user retention and feedback ratings. Experiment with prompt variants to improve helpfulness while maintaining safe responses.</p>"},{"location":"05_ai_applications/#example-code","title":"Example Code","text":"<pre><code>import openai\n\nchat_history = []\n\ndef chat(question):\n    chat_history.append({\"role\": \"user\", \"content\": question})\n    resp = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=chat_history)\n    answer = resp.choices[0].message.content\n    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n    return answer\n</code></pre> <p>After week five you will know how to plan and build LLM applications with a focus on evaluation, safety and user-centered design.</p> <p>Thorough monitoring and iterative prompt design will ensure your applications remain reliable as models and requirements evolve.</p>"},{"location":"05_ai_applications/#7-additional-resources","title":"7. Additional Resources","text":"<ul> <li>Papers on LLM application design, such as OpenAI's documentation and industry   case studies.</li> <li>Frameworks like LangChain and LlamaIndex provide utilities for rapid   prototyping and evaluation.</li> </ul>"},{"location":"06_ai_agent_langchain/","title":"AI Agent Development using LangChain","text":"<p>In week six we explore LangChain, a Python framework for building agents that chain multiple language model calls and tools together.</p>"},{"location":"06_ai_agent_langchain/#outline","title":"Outline","text":"<ul> <li>building chains</li> <li>tools and plugins</li> <li>agent memory and state</li> <li>multi-step agents</li> <li>evaluation techniques</li> <li>example project</li> <li>further exploration</li> </ul>"},{"location":"06_ai_agent_langchain/#1-building-chains","title":"1. Building Chains","text":"<p>A chain is a sequence of prompts, model invocations and actions. LangChain makes it easy to connect these steps so that the output of one becomes the input of the next.</p>"},{"location":"06_ai_agent_langchain/#memory-management","title":"Memory Management","text":"<p>Conversation memory objects store user messages and model responses. This allows an agent to maintain context over extended dialogues. Different memory types support summarization or token-limited histories. Persistent memory classes write interactions to disk or a database so agents can resume conversations even after a restart. Summarizing memory compresses long transcripts into embeddings or key sentences to fit within model context windows.</p>"},{"location":"06_ai_agent_langchain/#2-tools-and-plugins","title":"2. Tools and Plugins","text":"<p>LangChain integrates with many tools such as web search, code execution sandboxes and vector stores. Tools are invoked by the agent when it determines additional data is needed to answer a question.</p>"},{"location":"06_ai_agent_langchain/#custom-tools","title":"Custom Tools","text":"<p>Developers can define their own tools by writing a function and registering it with the agent. Tools may access internal APIs, databases or external services.</p>"},{"location":"06_ai_agent_langchain/#3-agent-memory-and-state","title":"3. Agent Memory and State","text":"<p>Agents maintain state across turns using different memory classes. Simple buffer memory stores the full conversation while summary memory compresses old exchanges to keep prompts short. Key-value memory can hold structured data such as the user\u2019s name or preferences for use in later steps.</p>"},{"location":"06_ai_agent_langchain/#4-multi-step-agents","title":"4. Multi-step Agents","text":"<p>Complex tasks often require retrieval, reasoning and action in sequence. For example an agent might search documentation, parse the results and then compose a concise answer. LangChain provides higher-level agent classes that handle decision making and tool selection.</p>"},{"location":"06_ai_agent_langchain/#debugging-and-tracing","title":"Debugging and Tracing","text":"<p>LangChain includes verbose logging and tracing utilities. These help inspect the reasoning process of the agent and diagnose unexpected behavior.</p>"},{"location":"06_ai_agent_langchain/#5-example-project","title":"5. Example Project","text":"<p>Build a question answering assistant that looks up information in a vector store before replying. The chain uses the following steps: 1. Receive the user question. 2. Retrieve relevant documents from a vector database. 3. Feed the documents and question to a language model. 4. Return the model\u2019s summarized answer along with references.</p>"},{"location":"06_ai_agent_langchain/#sample-code","title":"Sample Code","text":"<pre><code>from langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import FAISS\n\nvector_store = FAISS.load_local('docs_index')\nllm = OpenAI(model_name='gpt-3.5-turbo')\nqa = RetrievalQA(llm=llm, retriever=vector_store.as_retriever())\nresponse = qa.run('How do I install the package?')\nprint(response)\n</code></pre>"},{"location":"06_ai_agent_langchain/#6-evaluation-techniques","title":"6. Evaluation Techniques","text":"<p>Evaluate agents by logging intermediate steps and checking whether the chosen tools lead to correct answers. LangChain provides tracing utilities that export JSON traces for later analysis. You can also score final answers against a test set or human ratings.</p>"},{"location":"06_ai_agent_langchain/#7-further-exploration","title":"7. Further Exploration","text":"<ul> <li>Experiment with different chain structures such as sequential chains or parallel tool calls.</li> <li>Use the LangChain evaluation module to benchmark your agent\u2019s performance.</li> </ul> <p>By the end of week six you will be prepared to design complex LLM agents that use external tools and memory to solve real problems.</p> <p>The LangChain documentation includes many advanced patterns such as ReAct-style reasoning and planner-executor designs. Experiment with these templates to build agents that can perform multi-step research or workflow automation.</p>"},{"location":"07_llm_solutions/","title":"LLM Solutions","text":"<p>The final week surveys complete solutions built with language models across different industries. We discuss deployment considerations and real-world success stories.</p>"},{"location":"07_llm_solutions/#outline","title":"Outline","text":"<ul> <li>conversational systems</li> <li>summarization and question answering</li> <li>domain automation</li> <li>deployment considerations</li> <li>evaluation and analytics</li> <li>capstone project</li> </ul> <p>Throughout this week we connect the theory from earlier modules to real-world deployments. Each domain presents unique challenges around data privacy, latency and user experience, but common architectural patterns emerge.</p>"},{"location":"07_llm_solutions/#1-conversational-systems","title":"1. Conversational Systems","text":"<p>Chatbots and virtual assistants automate customer service and technical support. Effective systems combine retrieval (for factual accuracy) with dialogue management. Tools such as sentiment detection help tailor responses to user emotions.</p> <p>Large deployments often integrate with CRM platforms to fetch account details and update tickets automatically. Multi-turn dialogue management ensures context is preserved across sessions.</p>"},{"location":"07_llm_solutions/#2-summarization-and-question-answering","title":"2. Summarization and Question Answering","text":"<p>Organizations use LLMs to condense lengthy reports or answer natural language queries over internal documents. Combining RAG pipelines with prompt engineering yields concise, trustworthy results.</p> <p>Techniques like map-reduce summarization process long documents in chunks and then compose a final answer. Cross-encoders can rerank retrieved passages for better accuracy.</p>"},{"location":"07_llm_solutions/#3-domain-automation","title":"3. Domain Automation","text":"<p>In finance, language models analyze market reports and generate portfolio summaries. Healthcare applications include patient triage chatbots and medical coding assistance. In education, tutors generate practice questions and personalized feedback.</p> <p>Automation pipelines typically involve extraction prompts followed by downstream data processing. Integrating LLMs with existing business workflows requires careful handling of edge cases and fallback procedures.</p>"},{"location":"07_llm_solutions/#4-deployment-considerations","title":"4. Deployment Considerations","text":"<p>To run these solutions at scale we need secure infrastructure, monitoring and failover plans. Choosing between cloud-hosted APIs or self-hosted models depends on latency, cost and privacy requirements. Logging and analytics help track performance over time. Container orchestration platforms such as Kubernetes simplify scaling and enable rolling updates. When deploying sensitive models on-premises, encryption and role-based access control are essential to protect proprietary data.</p>"},{"location":"07_llm_solutions/#5-evaluation-and-analytics","title":"5. Evaluation and Analytics","text":"<p>Production systems must monitor both technical metrics and user satisfaction. Key metrics include response latency, accuracy on benchmark questions and conversation length. Analytics platforms such as Elastic or DataDog can collect logs for troubleshooting and trend analysis.</p> <p>Regular evaluation days allow teams to review logs and fine-tune prompts or retrievers. Over time, this data builds a feedback loop that guides future model updates and training.</p>"},{"location":"07_llm_solutions/#6-capstone-project","title":"6. Capstone Project","text":"<p>Students are encouraged to design a small end-to-end application incorporating retrieval, prompt engineering and an LLM agent. Presentations at the end of the course showcase the creativity and technical depth achieved during the seven weeks.</p>"},{"location":"07_llm_solutions/#7-case-studies","title":"7. Case Studies","text":"<ul> <li>E-commerce Chatbot: uses product embeddings to recommend items and answer   queries about inventory.</li> <li>Legal Document Summarizer: assists lawyers by extracting key clauses and   summarizing long contracts.</li> <li>Healthcare Assistant: triages symptoms and schedules appointments while   adhering to regulatory requirements.</li> <li>Educational Tutor: generates practice problems and adaptive feedback for   students preparing for exams.</li> </ul> <p>By studying these complete solutions you will understand how the concepts from earlier weeks combine to create robust, real-world products powered by language models.</p> <p>Continued experimentation will reveal new possibilities as LLM technology evolves, enabling innovative services across every sector.</p>"},{"location":"08_case_example_txn_reconcile/","title":"Case Example: Transaction reconciliation","text":""},{"location":"08_case_example_txn_reconcile/#account-ledger-reconciliation","title":"Account ledger reconciliation","text":"<p>The aim of the reconciliation process is to reconcile two account ledgers at a transaction level:</p> <ul> <li>source of truth: actual bank statements, which is always lagging due to the limitation of manual MFA for statement downloading. The actual bank statements lack tagging to user specific categories, budget accounts and taxonomy.</li> <li>digital twin: a user specific ledger, updated in real-time, with transactions tagged with user data model taxonomy.</li> </ul>"},{"location":"08_case_example_txn_reconcile/#motivation-and-aims-for-ai-automation","title":"Motivation and aims for AI automation","text":"<p>The motivation for automating with AI:</p> <ul> <li>Current process is <ul> <li>labor intensive, time-consuming</li> <li>Subject to human-error, fatigue</li> </ul> </li> <li>Largely deterministic and structured, making it a good candidate for AI-assisted automation</li> </ul>"},{"location":"08_case_example_txn_reconcile/#ai-rpa-augmented-design","title":"AI RPA Augmented design","text":"<ul> <li>LLMs complement, rather than replace, deterministic RPA</li> <li>Hybrid scoring (similarity + classification) balances flexibility and accuracy</li> <li>RAG-style repair suggestion helps bridge edge cases</li> <li>Manual override is essential for ambiguous or business-critical cases</li> <li>Session orchestrators using LangChain can manage multi-step workflows</li> <li>Feedback loop enables continual improvement of all agents</li> </ul>"},{"location":"08_case_example_txn_reconcile/#reconciliation-workflow","title":"Reconciliation workflow","text":"<ul> <li>Application load</li> <li>Statement load to DB</li> <li>Pre-processing</li> <li>Transaction matching</li> <li>Repair edits</li> <li>Review feedback</li> <li>Application close</li> </ul> <pre><code>graph TD\n  A[**User** Login &amp; Dashboard] --&gt; B[**User** Statement Upload]\n  B --&gt; C[**RPA** Parse &amp; Load to DB]\n  C --&gt; D[Preprocessing]\n  D --&gt; D1[**RPA** Input validation]\n  D --&gt; D2[**LLM Agent** sanitation check]\n  D --&gt; D3[**RPA** custom pre-processing]\n  D1 --&gt; E[Hybrid Transaction Matching]\n  D2 --&gt; E\n  D3 --&gt; E\n  E --&gt; K1[**RPA** Similarity Score]\n  E --&gt; K2[**ML** Trained Classifier Score]\n  K1 --&gt; L[**RPA** Weighted Match Score]\n  K2 --&gt; L\n  L --&gt; F[Generate Repair Edits]\n  F --&gt; G1[**RPA** Repair]\n  F --&gt; G2[**LLM Agent** Repair]\n  F --&gt; G3[**User** Repair]\n  G1 --&gt; H[**LLM Agent** Session Review]\n  G2 --&gt; H\n  G3 --&gt; H\n  H --&gt; I[**RPA** Training Feedback]\n  I --&gt; J[**RPA** Session Close &amp; Report Update]</code></pre>"},{"location":"08_case_example_txn_reconcile/#00-application-load","title":"00 Application load","text":"<p>The user logs in to the application and begins a new session, the state is updated and the application detects the current state and status of reconciliations by month and account. The application guides the user to next steps and issues to resolve if any</p>"},{"location":"08_case_example_txn_reconcile/#01-statement-upload","title":"01 Statement upload","text":"<p>User manually downloads statements as either PDF or Excel compatible (.csv, .xlsx) file format from the bank authenticated using MFA and uploads them using a file uploader UI feature that drops the files to a designated cloud storage location such as an AWS S3 bucket </p>"},{"location":"08_case_example_txn_reconcile/#02-statement-db-load","title":"02 Statement DB load","text":"<p>Robotic Process Automation (RPA) ingests, parses the statements, performs cleanup and formatting and uploads the transaction data to a structured SQL database. </p> <p>AI enhancement Although a fully RPA process without LLM may suffice for well structured statements, this process could optionally be enhanced with pre and post processing steps added with an LLM Agent to perform pre-upload checks and to review and resolve any processing errors in the first pass statement ingestion. </p> <p>Database The database can take many types of forms. On AWS, for example, could include RDS, Aurora or even an SQLite DB stored in S3 bucket</p>"},{"location":"08_case_example_txn_reconcile/#03-transaction-matching","title":"03 Transaction matching","text":"<p>The transaction matching is the main reconciliation workflow and can include multiple passes and methods, and update with learning as the program is exposed and trained from user input from past reconcilation cases.</p> <p>Input validation pre-processing RPA process performs basic pre-processing input validation for easily detectable data integrity issues such as NULL values in non-NULL fields, duplicate transactions or empty ledgers.</p> <p>LLM Agent sanitation pre-processing LLM Agent has a peek at the data to check for potential problematic cases, perform custom clean-up, formatting, tagging and pre-formatting transactions or run pre-defined clean-up operations not yet built into the RPA pre-processing.</p> <p>RPA Special cases pre-processing Custom pre-processing runs first by RPA process for pre-defined known special cases involving recurring transaction pairs and difficult to handle cases of many-to-many transaction pairs that have been either learned or manually defined by the user.</p> <p>transaction pre-processing: Individual transactions are pre-processed with standard NLP processing to format into standard features [date, account, amount, status, currency, payee, description] remove stopwords, POS tagging, lemmatization, and custom tagging from user-specific model (learned + user-defined)</p> <p>Hybrid classification model: transaction pairs are classified from a hybrid similarity match (no training required) + trained model as either matched or not [0, 1]. The two models (similarity, trained) generate independent scores and the final score is taken as a weighted combination of the two component scores. Over time, the weight of the trained model can be dynamically incremented as accuracy improvement from exposure to more use cases.</p> <p>Similarity model: The similarity model can utilize different methods of similarity matching, each with different trade-offs of computational cost and accuracy. Simple token matching can be performed at high speed and low computational costs, whereas sentence embedding, BERT models can add accuracy boost with a noticeable trade-off in computational costs.</p> <p>Trained classification model: There are many types of standard classification models with Decision tree and Random Forest a good first choice and Neural Networks and Multi-Layered Neural Networks as advanced options.</p>"},{"location":"08_case_example_txn_reconcile/#04-repair-edits-list","title":"04 Repair edits list","text":"<p>A list of repair edits is generated that will close the reconciliation gap between the ending balance between the reference source-of-truth and the digital twin ledgers.  Each transaction edit is either a REMOVE, ADD or UPDATE.  One fail-safe method for ensuring the process will always generate a solution is to begin with a naive set of edits: REMOVE ALL digital twin txns, ADD ALL source-of-truth, and then remove known matched transactions from the prior transaction matching step pair-wise that sum to zero, ensuring that each step of removed matched transactions preserves the condition of matched ending balance between source-of-truth and digital twin.</p> <p>RPA repair: An RPA process can automatically process repair edits to the digital twin some limited cases of qualified RPA-allowed repairs. Examples of allowed cases may include only amount update by &lt; +/- some threshold amount, or for foreign currency transactions or variable bills which are expected to vary from a range of expected amount in some range.</p> <p>LLM Agent review: An LLM Agent can review the residual edits and follow a set of guidelines to apply repairs not caught by the RPA process, and for residual repairs to propose edits to the user which do not meet the guidelines for Agent repair. These guidelines can be seeded by the user and updated with learning and feedback by the AI Agent.</p> <p>Manual repair: For any residual edits that do not qualify for either RPA or Agent repair, the user is prompted to manually repair either by manually matching transaction with instruction to update the digital twin to match the source-of-truth amount, or by manually updating the digital twin transaction. For example, in case where the transactions were not matched due to a miss-spelling or incorrect transaction category tagging, the user may decide to update the tag and regeneration the edits list in order to prevent corruption of the training model by feeding it bad training feedback cases.  </p>"},{"location":"08_case_example_txn_reconcile/#05-agent-session-review","title":"05 Agent session review","text":"<p>An LLM Agent is prompted with the session details, and plans workflow for model, guideline updates.</p>"},{"location":"08_case_example_txn_reconcile/#06-training-feedback","title":"06 Training feedback","text":"<p>The complete set of matched transactions and edit repairs is stored to train each of the models involved, the classifier, the RPA repair and the LLM Agent guide, with specific workflows triggered by the output of the Agent session review.</p>"},{"location":"08_case_example_txn_reconcile/#07-session-close","title":"07 Session close","text":"<p>The downstream reports are updated from the updated digital twin and the session status is updated as closed. Any cleanup and logging are recorded to log and record session activity.</p>"},{"location":"08_case_example_txn_reconcile/#application-architecture","title":"Application Architecture","text":"<p>Frontend:</p> <ul> <li>User dashboard</li> <li>File upload UI (PDF/CSV/XLSX)</li> <li>Manual repair UI for unmatched or uncertain items</li> </ul> <p>Session Orchestrator:</p> <ul> <li>Maintains session state</li> <li>Plans workflows</li> <li>Routes feedback to the right learning modules (Classifier, LLM agent, RPA)</li> </ul> <p>Model Trainer:</p> <ul> <li>incorprates feedback to train and tune models</li> </ul> <p>Backend Functional Modules:</p> <ul> <li>RPA Parser: Parses bank statements</li> <li>Data Loader: Uploads structured transactions to SQL DB</li> <li>Digital twin API: CRUD DB operations to Digital Twin DB</li> </ul> <p>Preprocessing Engine:</p> <ul> <li>RPA input validation</li> <li>LLM sanitation and formatting</li> <li>Special-case handlers</li> </ul> <p>Matching Engine:</p> <ul> <li>Similarity-based matcher (token, embedding, BERT)</li> <li>Trained classifier (Random Forest / MLP)</li> <li>weighted score fusion module</li> <li>weight adapter from experience + classification accuracy</li> </ul> <p>Repair Engine:</p> <ul> <li>Deduces minimal edit set to align ledgers</li> <li>RPA auto-fixes</li> <li>LLM repair agent</li> <li>Manual override</li> </ul>"},{"location":"08_case_example_txn_reconcile/#tech-stack","title":"Tech stack","text":"<ul> <li>Cloud: GitHub, AWS S3, EC2, Lambda</li> <li>Database: Aurora PostgreSQL or SQLite in S3</li> <li>Embedding Model: all-MiniLM-L6-v2</li> <li>Vector Store: FAISS or Qdrant</li> <li>LLM Orchestration: LangChain agent with tool access and session controller</li> <li>Document loader: PyPDF, CSV parser, or structured ingest from databases</li> </ul>"},{"location":"08_case_example_txn_reconcile/#code-snippet-session-orchestration","title":"Code Snippet Session Orchestration","text":"<p>This snippet provides a simplified conceptual overview of the overall orchestration and not an actual implementation source code</p> <pre><code>from langchain.agents import AgentExecutor, initialize_agent, Tool\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.llms import OpenAI\nfrom session_utils import *\n\n# Step 1: Define tools the agent can use\ntools = [\n    Tool(name=\"load_ledger\", func=load_user_ledger, description=\"Load user ledger\"),\n    Tool(name=\"load_bank_stmt\", func=load_bank_statement, description=\"Load bank statement\"),\n    Tool(name=\"preprocess\", func=preprocess, description=\"Preprocess both ledgers\"),\n    Tool(name=\"match_txns\", func=match_txns, description=\"Match transactions\"),\n    Tool(name=\"generate_repairs\", func=generate_repairs, description=\"Generate repair edits\"),\n    Tool(name=\"rpa_repair\", func=run_rpa_repairs, description=\"Run RPA repairs\"),\n    Tool(name=\"llm_repair\", func=run_llm_repairs, description=\"Run LLM repairs\"),\n    Tool(name=\"manual_review\", func=manual_review, description=\"Prompt user for unmatched transactions\"),\n    Tool(name=\"check_balances\", func=check_balances, description=\"Check if ledgers are reconciled\"),\n    Tool(name=\"process_feedback\", func=process_feedback, description=\"Log and route feedback\"),\n    Tool(name=\"update_model\", func=update_model, description=\"Update models with feedback\"),\n    Tool(name=\"close_session\", func=close_session, description=\"Close the reconciliation session\"),\n]\n\n# Step 2: Build the agent\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n# Step 3: Execute\nagent.run(\"Begin a session and reconcile the two ledgers. Continue until complete.\")\n</code></pre>"},{"location":"08_case_example_txn_reconcile/#other-use-cases","title":"Other use cases","text":"<ol> <li>Case complaint classification</li> <li>Job search profile matching</li> <li>Personal finances coach</li> </ol>"}]}