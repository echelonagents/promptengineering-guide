# Natural Language Processing

This week introduces fundamental techniques for working with human language data. We explore how raw text is prepared, represented and embedded for downstream tasks.

## Text Preprocessing
- Normalization and cleaning of text
- Tokenization strategies: word, subword and byte pair encoding
- Handling stop words and punctuation

## Word Representations
Classical methods such as bag-of-words and TFâ€‘IDF are covered before moving to neural embeddings like word2vec and GloVe. These representations capture semantic relationships between terms.

## Embeddings and Semantic Search
Modern models produce vector embeddings that allow similarity search and clustering. We discuss cosine similarity, vector space properties and how embeddings power search applications.

## Practical Exercise
Using Python libraries (NLTK, spaCy), process a small corpus, create embeddings and perform a similarity query.
