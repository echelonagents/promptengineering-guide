
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../01_natural_language_processing/">
      
      
        <link rel="next" href="../03_rag_embedding/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Large Language Models - Prompt Engineering Guide</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#large-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Prompt Engineering Guide" class="md-header__button md-logo" aria-label="Prompt Engineering Guide" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Prompt Engineering Guide
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Large Language Models
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Prompt Engineering Guide" class="md-nav__button md-logo" aria-label="Prompt Engineering Guide" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Prompt Engineering Guide
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_natural_language_processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Large Language Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Large Language Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#outline" class="md-nav__link">
    <span class="md-ellipsis">
      Outline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-model-families" class="md-nav__link">
    <span class="md-ellipsis">
      1. Model Families
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Model Families">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#proprietary-models" class="md-nav__link">
    <span class="md-ellipsis">
      Proprietary Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#open-source-alternatives" class="md-nav__link">
    <span class="md-ellipsis">
      Open Source Alternatives
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      2. Transformer Architecture
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Transformer Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#autoregressive-training" class="md-nav__link">
    <span class="md-ellipsis">
      Autoregressive Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaling-laws" class="md-nav__link">
    <span class="md-ellipsis">
      Scaling Laws
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-tokenization-and-vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      3. Tokenization and Vocabulary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-training-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      4. Training Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-using-hosted-apis" class="md-nav__link">
    <span class="md-ellipsis">
      5. Using Hosted APIs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Using Hosted APIs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    <span class="md-ellipsis">
      Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tips" class="md-nav__link">
    <span class="md-ellipsis">
      Tips
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-evaluation-and-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      6. Evaluation and Fine-tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Evaluation and Fine-tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-efficient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter-Efficient Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-ethical-and-practical-concerns" class="md-nav__link">
    <span class="md-ellipsis">
      7. Ethical and Practical Concerns
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-hands-on-code-example" class="md-nav__link">
    <span class="md-ellipsis">
      8. Hands-on Code Example
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_rag_embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG, Embedding and Vector Databases
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_prompt_engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Engineering
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_ai_applications/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Applications
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_ai_agent_langchain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Agents using LangChain
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_llm_solutions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Solutions
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#outline" class="md-nav__link">
    <span class="md-ellipsis">
      Outline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-model-families" class="md-nav__link">
    <span class="md-ellipsis">
      1. Model Families
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Model Families">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#proprietary-models" class="md-nav__link">
    <span class="md-ellipsis">
      Proprietary Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#open-source-alternatives" class="md-nav__link">
    <span class="md-ellipsis">
      Open Source Alternatives
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      2. Transformer Architecture
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Transformer Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#autoregressive-training" class="md-nav__link">
    <span class="md-ellipsis">
      Autoregressive Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaling-laws" class="md-nav__link">
    <span class="md-ellipsis">
      Scaling Laws
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-tokenization-and-vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      3. Tokenization and Vocabulary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-training-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      4. Training Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-using-hosted-apis" class="md-nav__link">
    <span class="md-ellipsis">
      5. Using Hosted APIs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Using Hosted APIs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    <span class="md-ellipsis">
      Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tips" class="md-nav__link">
    <span class="md-ellipsis">
      Tips
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-evaluation-and-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      6. Evaluation and Fine-tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Evaluation and Fine-tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-efficient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter-Efficient Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-ethical-and-practical-concerns" class="md-nav__link">
    <span class="md-ellipsis">
      7. Ethical and Practical Concerns
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-hands-on-code-example" class="md-nav__link">
    <span class="md-ellipsis">
      8. Hands-on Code Example
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="large-language-models">Large Language Models</h1>
<p>Modern large language models (LLMs) such as GPT-4, Claude and open-source alternatives are built upon the transformer architecture. This week provides a deep dive into how these models are trained and how we can use them effectively.</p>
<h2 id="outline">Outline</h2>
<ul>
<li>model families and providers</li>
<li>transformer architecture basics</li>
<li>tokenization and vocabulary</li>
<li>training algorithms</li>
<li>using hosted APIs</li>
<li>evaluation and fine-tuning</li>
<li>ethical and practical concerns</li>
<li>hands-on code example</li>
</ul>
<h2 id="1-model-families">1. Model Families</h2>
<p>LLM providers fall into two broad categories: proprietary offerings like OpenAI’s GPT series or Anthropic’s Claude, and open models such as Llama or Mistral. Each family offers different licensing terms and API interfaces, but the underlying neural architecture shares common principles.</p>
<h3 id="proprietary-models">Proprietary Models</h3>
<ul>
<li><strong>GPT-3/4</strong>: Developed by OpenAI, these models are known for their extensive training data and high-quality generation.</li>
<li><strong>Claude</strong>: Anthropic’s assistant focuses on safety and steerable responses.</li>
</ul>
<h3 id="open-source-alternatives">Open Source Alternatives</h3>
<ul>
<li><strong>Llama</strong> from Meta and <strong>Mistral</strong> from independent labs show that smaller, efficient models can achieve competitive performance when fine-tuned on domain-specific data. Running open models locally gives greater control over data privacy and cost.</li>
</ul>
<h2 id="2-transformer-architecture">2. Transformer Architecture</h2>
<p>The transformer revolutionized NLP by introducing self-attention mechanisms. Each layer computes queries, keys and values to weigh the importance of surrounding tokens. Positional encodings supply order information, while residual connections and layer normalization aid optimization.</p>
<h3 id="autoregressive-training">Autoregressive Training</h3>
<p>LLMs are typically trained to predict the next token in a sequence. This objective encourages the model to learn grammar, semantics and even factual knowledge from the training corpus.</p>
<h3 id="scaling-laws">Scaling Laws</h3>
<p>Research shows that model performance scales predictably with data, parameter count and compute budget. Understanding these relationships helps when deciding between a smaller local model and a large hosted service.</p>
<h2 id="3-tokenization-and-vocabulary">3. Tokenization and Vocabulary</h2>
<p>Tokenizers split text into manageable pieces. Byte-Pair Encoding (BPE) and WordPiece are common algorithms that build a vocabulary of subword units. Each token is mapped to an integer ID so it can be processed by the model. The vocabulary typically includes special symbols like <code>&lt;pad&gt;</code> for padding sequences and <code>&lt;eos&gt;</code> for marking the end of a sentence. Proper tokenization yields consistent sequence lengths and reduces out-of-vocabulary issues.</p>
<h2 id="4-training-algorithms">4. Training Algorithms</h2>
<p>Training begins with massive text corpora that are shuffled and broken into fixed-length sequences. The transformer processes each batch while the optimizer updates weights via gradient descent. Strategies such as gradient accumulation, mixed-precision training and data parallelism enable scaling to billions of parameters. Many teams further align the model with Reinforcement Learning from Human Feedback (RLHF), which uses human preference scores and Proximal Policy Optimization to refine responses.</p>
<h2 id="5-using-hosted-apis">5. Using Hosted APIs</h2>
<p>Most developers access LLMs through web APIs. When sending prompts, it’s important to manage context length, system instructions and rate limits. Providers like OpenAI return responses in JSON format with metadata. Error handling ensures robust applications.</p>
<h3 id="example">Example</h3>
<pre><code class="language-python">import openai
openai.api_key = &quot;YOUR_KEY&quot;
response = openai.ChatCompletion.create(
    model=&quot;gpt-4&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain transformers&quot;}]
)
print(response.choices[0].message.content)
</code></pre>
<h3 id="tips">Tips</h3>
<ul>
<li>Cache frequent prompts to reduce latency and cost.</li>
<li>Monitor token usage to avoid unexpected bills.</li>
<li>Evaluate model outputs for bias or inappropriate content.</li>
</ul>
<h2 id="6-evaluation-and-fine-tuning">6. Evaluation and Fine-tuning</h2>
<p>While base models provide impressive capabilities, performance often improves with fine-tuning on task-specific data. Evaluation metrics such as perplexity, BLEU or human preference scores help determine whether fine-tuning is necessary.</p>
<h3 id="parameter-efficient-methods">Parameter-Efficient Methods</h3>
<p>Techniques like LoRA (Low-Rank Adaptation) and prompt tuning modify only a small subset of model parameters or the input embeddings, enabling efficient adaptation even on modest hardware.</p>
<h2 id="7-ethical-and-practical-concerns">7. Ethical and Practical Concerns</h2>
<p>Large models inherit biases present in their training data. It is critical to monitor for unfair or harmful outputs. Data privacy must be considered when sending user content to hosted services. Finally, the energy consumption required to train these models raises environmental concerns. Organizations should weigh these factors when deciding on adoption.</p>
<h2 id="8-hands-on-code-example">8. Hands-on Code Example</h2>
<p>The snippet below shows how a small transformer model can be fine-tuned using the <code>transformers</code> library from Hugging Face.</p>
<pre><code class="language-python">from datasets import load_dataset
from transformers import (AutoTokenizer, AutoModelForCausalLM,
                          Trainer, TrainingArguments)

tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

def tokenize(batch):
    return tokenizer(batch['text'], truncation=True, padding='max_length')

data = load_dataset('wikitext', 'wikitext-2-raw-v1')
tokenized = data['train'].map(tokenize, batched=True)

args = TrainingArguments('out', per_device_train_batch_size=2, num_train_epochs=1)
trainer = Trainer(model=model, args=args, train_dataset=tokenized)
trainer.train()
</code></pre>
<p>By the end of week two you will be able to describe the architecture of modern LLMs, interact with them via APIs and understand the trade-offs between proprietary and open-source solutions.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>